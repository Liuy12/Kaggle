data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
head(trainSA)
trainSA <- trainSA[,-c(1,4,5)]
testSA <- testSA[,-c(1,4,5)]
?train
modelfit <- train(chd~.,method='glm',family='binomial',data=trainSA)
modelfit$finalModel
pred_tr <- predict(modelfit, trainSA)
pred_ts <- predict(modelfit, testSA)
pred_tr
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(trainSA$chd,pred_tr)
missClass(testSA$chd,pred_ts)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.test
class(vowel.test$y)
as.factor(vowel.test$y)
vowel.test$y <- as.factor(vowel.test$y)
vowel.train$y <- as.factor(vowel.train$y)
?varImp
modelfit <- train(y~., method='rf',data=vowel.train)
varImp(modelfit$finalModel)
ordervarImp(modelfit$finalModel)
order(varImp(modelfit$finalModel)
)
order(varImp(modelfit$finalModel),decreasing = F)
order(varImp(modelfit$finalModel),decreasing = T)
set.seed( 33833)
modelfit <- train(y~., method='rf',data=vowel.train)
varImp(modelfit$finalModel)
order(varImp(modelfit$finalModel),decreasing = T)
pred <- predict(modelfit,vowel.test)
pred
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
Intrain <- createDataPartition(segmentationOriginal$Class,p = 0.75)[[1]]
training <- segmentationOriginal[Intrain,]
testing <- segmentationOriginal[-Intrain,]
set.seed(125)
modelfit <- train(Class~.,method = 'rpart',data = training)
library(rattle)
fancyRpartPlot(modelfit$finalModel)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
trainSA <- trainSA[,-c(1,4,5)]
testSA <- testSA[,-c(1,4,5)]
modelfit <- train(chd~.,method='glm',family='binomial',data=trainSA)
modelfit$finalModel
pred_tr <- predict(modelfit, trainSA)
pred_ts <- predict(modelfit, testSA)
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(trainSA$chd,pred_tr)
missClass(testSA$chd,pred_ts)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.test$y <- as.factor(vowel.test$y)
vowel.train$y <- as.factor(vowel.train$y)
set.seed( 33833)
modelfit <- train(y~., method='rf',data=vowel.train)
pred <- predict(modelfit,vowel.test)
order(varImp(modelfit$finalModel),decreasing = T)
rm(list = ls())
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.test$y <- as.factor(vowel.test$y)
vowel.train$y <- as.factor(vowel.train$y)
modelfit <- train(y~., method='rf',data=vowel.train)
set.seed( 33833)
modelfit <- train(y~., method='rf',data=vowel.train)
modelfit1 <- train(y~.,method='gbm',data=vowel.train)
pred <- predict(modelfit,vowel.test)
pred1 <- predict(modelfit1,vowel,test)
pred1 <- predict(modelfit1,vowel.test)
pred
confusionMatrix(vowel.test$y,pred)
confusionMatrix(vowel.test$y,pred1)
subset <- which(pred==pred1)
confusionMatrix(vowel.test$y[subset],pred[subset])
rm(list=ls())
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.sedd(62433)
set.seed(62433)
modelfit1 <- train(diagnosis~., method='rf',data=training)
modelfit2 <- train(diagnosis~., method='gbm',data=training)
modelfit3 <- train(diagnosis~., method='lda',data=training)
modelfit3$finalModel
pred1 <- predict(modelfit1,testing)
pred2 <- predict(modelfit2,testing)
pred3 <- predict(modelfit3,testing)
df <- data.frame(rf=pred1,gbm=pred2, lda = pred3,diagnosis = testing$diagnosis)
combmodel <- train(diagnosis~.,method='rf',data=df)
df
compred <- predict(combmodel,df)
compred
compred <- predict(combmodel,df[,-4])
compred
confusionMatrix(testing$diagnosis,pred1)
confusionMatrix(testing$diagnosis,pred2)
confusionMatrix(testing$diagnosis,pred3)
confusionMatrix(testing$diagnosis,compred)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
modelfit <- train(CompressiveStrength~., method='lasso',data=training)
set.seed(233)
modelfit <- train(CompressiveStrength~., method='lasso',data=training)
?plot.enet
data(diabetes)
attach(diabetes)
object <- enet(x,y,lambda=1)
par(mfrow=c(2,2))
plot(object)
plot(object,xvar= 'penalty')
object$penalty
diabetes$x
head(diabetes$y)
plot.enet(modelfit)
plot.enet(modelfit$finalModel)
plot.enet(modelfit$finalModel)
plot.enet(modelfit$finalModel, xvar = 'penalty')
install.packages("lubridate")
library(lubridate)  # For year() function below
dat = read.csv("~/Desktop/gaData.csv")
dat = read.csv("C:/Users/liuy12.UTHSCSA/Desktop/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
head(training$visitsTumblr)
tstrain
head(training)
?bats
install.packages("forecast")
library(forecast)
?bats
modelfit <- bats(tstrain)
headtesting$visitsTumblr
head(testing$visitsTumblr)
length(testing$visitsTumblr)
tstest = ts(testing$visitsTumblr)
head(tstest)
head(tstrain)
head(training)
pred <- predict(modelfit,tstest)
plot(forecast(modelfit))
pred <- forecast(modelfit)
?forcast
?forecast
pred <- forecast(modelfit,level=95)
head(pred$mean)
head(pred$fitted)
head(pred$x)
head(pred$level)
accuracy(pred,tstest)
training = dat[year(dat$date) < 2011,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
tstest = ts(testing$visitsTumblr)
library(forecast)
modelfit <- bats(tstrain)
pred <- forecast(modelfit,level=95)
accuracy(pred,tstest)
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2012,]
tstrain = ts(training$visitsTumblr)
tstest = ts(testing$visitsTumblr)
library(forecast)
modelfit <- bats(tstrain)
pred <- forecast(modelfit,level=95)
accuracy(pred,tstest)
range(year(dat$date))
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
head(training)
head(testing)
tstrain = ts(training$visitsTumblr)
tstest = ts(testing$visitsTumblr)
library(forecast)
modelfit <- bats(tstrain)
pred <- forecast(modelfit)
accuracy(pred,tstest)
accuracy(pred,testing)
accuracy(pred,tstest)
?window
window(tstest)
window(tstrain)
modelfit <- bats(window(tstrain))
pred <- forecast(modelfit)
accuracy(pred,window(tstest))
tstest
tstest@tsp
tstest@.Data
tsdat = ts(dat$visitsTumblr)
tsdat
tstrain = window(tsdat,start = 1, end= 365)
tstest = window(tsdat, start = 366,end=600)
modelfit <- bats(tstrain)
pred <- forecast(modelfit)
accuracy(pred,tstest)
plot(forecast(modelfit))
pred$upper
pred <- forecast(modelfit,h = 300)
plot(forecast(modelfit))
plot(pred)
lines(tstest)
modelfit <- bats(tstrain)
pred <- forecast(modelfit,h = 235)
accuracy(pred,tstest)
plot(pred)
lines(tstest)
rm(list = ls())
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
names(getModelInfo())
library(e1071)
?svm
modelfit <- svm(CompressiveStrength~., data = training)
pred <- predict(modelfit,testing)
pred
confusionMatrix(testing$CompressiveStrength,pred)
head(testing$CompressiveStrength)
pred
sqrt(mean((pred-testing$CompressiveStrength)^2))
rm(list=ls())
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
modelfit1 <- train(diagnosis~., method='rf',data=training)
modelfit2 <- train(diagnosis~., method='gbm',data=training)
modelfit3 <- train(diagnosis~., method='lda',data=training)
pred1 <- predict(modelfit1,testing)
confusionMatrix(testing$diagnosis,pred1)
pred2 <- predict(modelfit2,testing)
confusionMatrix(testing$diagnosis,pred2)
pred3 <- predict(modelfit3,testing)
confusionMatrix(testing$diagnosis,pred3)
load("C:/Users/liuy12.UTHSCSA/Dropbox/Machine learning/data.RData")
head(training$Cover_Type)
training$Cover_Type <- as.factor(training$Cover_Type)
head(training$Cover_Type)
head(paste('C',training$Cover_Type,sep=''))
training$Cover_Type <- as.factor(paste('C',training$Cover_Type,sep=''))
head(paste('C',training$Cover_Type,sep=''))
load("C:/Users/liuy12.UTHSCSA/Dropbox/Machine learning/data.RData")
library('lumi')
?lumiN
gc()
library(affy)
?normalize.loess
normalize.loess
?boxplot
?p.adjust()
gc()
library('caret')
?trainControl
?predict
?predict.train
gc()
temp <- (-2:8)
temp
10^temp
temp <- -2:8
temp1 <- -5:-1
tuneGrid_svm <- expand.grid(.C = 10^temp),
.sigma = 10^temp1
)
temp <- -2:8
temp1 <- -5:-1
tuneGrid_svm <- expand.grid(.C = 10^temp,
.sigma = 10^temp1)
tuneGrid_svm
?power
?power.t.test
require("mosaic")
install.packages('nnet')
library('nnet')
?nnet
?rnbinom
?save
install.packages('elmNN')
library('elmNN')
?elmNN
2^10
2^8
2^c(8:18)
?setClass
library(C50)
?C5.0
?setClass
install.packages('mboost')
library('mboost')
?gamboost
head(training_trans)
install.packages('glmnet')
library('glmnet')
gc()
library('glmnet')
?glmnet
?setReplaceMethod
load("C:/Users/liuy12.UTHSCSA/Dropbox/Machine learning/data1.rdata")
install.packages(adabag)
install.packages('adabag')
library('adabag')
?adaboost.M1
colnames(training_trans)
plot(training_trans$Soil, training_trans$Elevation)
hist(training_trans$Elevation)
hist(training$Elevation)
hist(training$Aspect)
modelfit_rf_class1
modelfit_rf_class1$finalModel$importance
hist(training$Slope)
hist(training$Horizontal_Distance_To_Hydrology)
min(training)
min(training[1:40,])
min(training[,1:40])
hist(training$Vertical_Distance_To_Hydrology)
hist(training$Hillshade_9am)
hist(training$Hillshade_Noon)
hist(training$Hillshade_3pm)
hist(training$Horizontal_Distance_To_Fire_Points)
hist(training$Wilderness_Area1)
library('kernlab')
install.packages('kernlab')
library('kernlab')
load("C:/Users/liuy12.UTHSCSA/Dropbox/Machine learning/data1.rdata")
colnames(training_trans)
hist(training_trans$Elevation)
hist(training$Elevation)
summary(training$Elevation)
hist(training$Elevation, breaks=1000)
hist(training$Elevation, breaks=1000)
summary(training$Aspect)
hist(training$Aspect)
hist(training$Aspect, breaks=1000)
hist(training$Slope)
hist(training$Slope, breaks=1000)
hist(training$Slope, breaks=100)
hist(training$Slope, breaks=10)
hist(training$Slope, breaks=50)
hist(training$Horizontal_Distance_To_Hydrology, breaks=50)
hist(training$Vertical_Distance_To_Hydrology, breaks=50)
summary(training$Horizontal_Distance_To_Hydrology)
summary(training$Vertical_Distance_To_Hydrology)
gc()
summary(training$Horizontal_Distance_To_Roadways)
hist(training$Horizontal_Distance_To_Roadways, breaks=50)
hist(training$Hillshade_9am, breaks=50)
head(training$Hillshade_9am)
hist(training$Hillshade_Noon, breaks=50)
hist(training$Hillshade_3pm, breaks=50)
hist(training$Horizontal_Distance_To_Fire_Points, breaks=50)
library(ggplot2)
ggplot() + geom_point(aes(x=Elevation, y= Aspect, colour=Cover_Type))
ggplot() + geom_point(aes(x=Elevation, y= Aspect, colour=Cover_Type), data= training)
modelfit_rf_class2
modelfit_rf_class1
modelfit_rf_class1$finalModel$importance
modelfit_rf_class
colnames(training_trans)
ggplot() + geom_point(aes(x=Horizontal_Distance_To_Roadways, y= Horizontal_Distance_To_Fire_Points, colour=Cover_Type), data= training)
ggplot() + geom_point(aes(x=Elevation, y= Horizontal_Distance_To_Roadways, colour=Cover_Type), data= training)
ggplot() + geom_point(aes(x=Elevation, y= Horizontal_Distance_To_Fire_Points, colour=Cover_Type), data= training)
ggplot() + geom_point(aes(x=Elevation, y= Vertical_Distance_To_Hydrology, colour=Cover_Type), data= training)
Combined_hdistance <- with(training, Horizontal_Distance_To_Roadways +
Horizontal_Distance_To_Fire_Points +
Horizontal_Distance_To_Hydrology)
head(Combined_hdistance)
ggplot() + geom_point(aes(x=Elevation, y= Combined_hdistance, colour=Cover_Type), data= training)
head(training$Horizontal_Distance_To_Roadways)
ggplot() + geom_point(aes(x=Elevation, y= Horizontal_Distance_To_Hydrology, colour=Cover_Type), data= training)
Euclid <- with( training, sqrt(Horizontal_Distance_To_Hydrology^2 + Vertical_Distance_To_Hydrology^2))
ggplot() + geom_point(aes(x=Elevation, y= Euclid, colour=Cover_Type), data= training)
tail(Euclid)
ggplot() + geom_point(aes(x=Elevation, y= Hillshade_9am, colour=Cover_Type), data= training)
ggplot() + geom_point(aes(x=Elevation, y= Hillshade_Noon, colour=Cover_Type), data= training)
ggplot() + geom_point(aes(x=Elevation, y= Hillshade_3pm, colour=Cover_Type), data= training)
with(training, length(Hillshade_3pm))
with(training, length(which(Hillshade_3pm==0)))
library('caret')
?varImp
?filterVarImp
Euclid_scaled <- scale(Euclid)
head(Euclid)
head(t(Euclid))
Euclid_scaled <- scale(t(Euclid))
head(Euclid)
?scale
Euclid_scaled <- scale(as.matrix(Euclid))
head(Euclid_scaled)
training1 <- training_scaled
training1$Euclid <- Euclid_scaled
colnames(training1)
filterVarImp(training1[,-55], training1[,55])
install.packages(pROC)
source('http://www.bioconductor.org/biocLite.R')
biocLite('pROC')
filterVarImp(training1[,-55], training1[,55])
Euclid <- with( training_scaled, sqrt(Horizontal_Distance_To_Hydrology^2 + Vertical_Distance_To_Hydrology^2))
Euclid <- with( training_scaled, sqrt(Horizontal_Distance_To_Hydrology^2 + Vertical_Distance_To_Hydrology^2))
head(Euclid)
Euclid_scaled <- scale(Euclid)
head(Euclid_scaled)
training1 <- training_scaled
training1$Euclid <- Euclid_scaled
filterVarImp(training1[,-55], training1[,55])
modelfit_rf_class$finalModel$importance
ggplot() + geom_point(aes(x=Elevation, y= Horizontal_Distance_To_Hydrology, colour=Cover_Type), data= training)
ggplot() + geom_point(aes(x=Elevation, y= Vertical_Distance_To_Hydrology, colour=Cover_Type), data= training)
gc()
ggplot() + geom_point(aes(x=Elevation- Horizontal_Distance_To_Hydrology, y= Horizontal_Distance_To_Hydrology, colour=Cover_Type), data= training)
ggplot() + geom_point(aes(x=Elevation- 0.2*Horizontal_Distance_To_Hydrology, y= Horizontal_Distance_To_Hydrology, colour=Cover_Type), data= training)
ggplot() + geom_point(aes(x=Elevation- 0.4*Horizontal_Distance_To_Hydrology, y= Horizontal_Distance_To_Hydrology, colour=Cover_Type), data= training)
ggplot() + geom_point(aes(x=Elevation- Vertical_Distance_To_Hydrology, y= Vertical_Distance_To_Hydrology, colour=Cover_Type), data= training)
training1$newfeature1 <- newfeature1
training1$newfeature2 <- newfeature2
newfeature1 <- scale(with (training, Elevation- 0.2*Horizontal_Distance_To_Hydrology))
newfeature2 <- scale(with (training, Elevation- Vertical_Distance_To_Hydrology))
training1$newfeature1 <- newfeature1
training1$newfeature2 <- newfeature2
filterVarImp(training1[,-55], training1[,55])
ggplot() + geom_point(aes(x=Horizontal_Distance_To_Hydrology, y= Vertical_Distance_To_Hydrology, colour=Cover_Type), data= training)
ggplot() + geom_point(aes(x=Horizontal_Distance_To_Roadways, y= Horizontal_Distance_To_Fire_Points, colour=Cover_Type), data= training)
ggplot() + geom_point(aes(x=Horizontal_Distance_To_Roadways, y= Horizontal_Distance_To_Hydrology, colour=Cover_Type), data= training)
gc()
gc()
library('randomForest')
?rfImpute
?scale
setwd('../Dropbox/gitrepository/Kaggle/Forest classification/')
training <- read.csv('train.csv',header=T,row.names=1)
testing <- read.csv('test.csv',header=T,row.names=1)
hist(testing$Hillshade_9am)
hist(testing$Hillshade_Noon)
hist(testing$Hillshade_3pm)
hist(training$Hillshade_3pm)
Wilderness <- factor(c(paste('wilder',1:4,sep='')))
Soiltype <- factor(c(paste('soil',1:40,sep='')))
train_subset <- training[,15:54]
train_subset1 <- training[,11:14]
index_soil <- apply(train_subset,1,function(i) which(i==1))
index_wild <- apply(train_subset1,1,function(i) which(i==1))
training_trans <- training[1:10]
training_trans$Wild <- Wilderness[index_wild]
training_trans$Soil <- Soiltype[index_soil]
training_trans[,1:10] <- train_subset2_scaled
training_trans$Cover_Type <- training$Cover_Type
train_subset2 <- training[,c(1:10, 56,57)]
mean_subset2 <- apply(train_subset2,2,mean)
sd_subset2 <- apply(train_subset2,2,sd)
train_subset2_scaled <- sweep(train_subset2,2,mean_subset2,'-')
train_subset2_scaled <- sweep(train_subset2_scaled,2,sd_subset2,'/')
training_scaled <- training
training_scaled[,c(1:10, 56,57)] <- train_subset2_scaled
table(training_trans$Wild)
with(training, table(Cover_Type, Wild))
with(training, table(Cover_Type, wild))
with(training_trans, table(Cover_Type, Wild))
with(training_trans, table(Cover_Type, Soil))
testing_trans <- testing
testing_subset1 <- testing[,15:54]
testing_subset2 <- testing[,11:14]
index_soil <- apply(testing_subset1,1,function(i) which(i==1))
index_wild <- apply(testing_subset2,1,function(i) which(i==1))
testing_trans$Wild <- Wilderness[index_wild]
testing_trans$Soil <- Soiltype[index_soil]
with(training_trans, table(Cover_Type, Soil))
with(testing_trans, table(Cover_Type, Soil))
with(testing_trans, table( Soil))
nrow(testing)
?gbm
library('gbm')
install.packages('gbm')
install.packages("gbm")
library('gbm')
?gbm
?pnbinom
